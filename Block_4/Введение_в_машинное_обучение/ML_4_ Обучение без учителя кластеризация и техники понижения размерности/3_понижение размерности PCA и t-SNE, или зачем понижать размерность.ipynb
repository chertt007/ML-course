{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9/13   9. PCA и t-SNE, или зачем понижать размерность? ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущих юнитах мы работали преимущественно в двухмерном или трёхмерном пространстве. Наши объекты описывались двумя или тремя признаками. Однако зачастую в реальных задачах объекты описываются куда большим числом признаков, а значит, работать приходится в многомерном пространстве, которое невозможно визуализировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трудности могут возникнуть, если:\n",
    "\n",
    "нужно визуализировать результаты кластеризации, а пространство признаков многомерное.\n",
    "нужно обучить модель, а объект описывается большим количеством признаков. Эти признаки могут быть избыточными или малоинформативными, что приведёт к плохим результатам при обучении модели.\n",
    "Избежать этих проблем поможет снижение размерности данных.\n",
    "\n",
    "→ Мы встречались с уменьшением размерности ранее, например когда рассматривали спектральную кластеризацию. В данном юните мы поближе познакомимся с тем, как работают алгоритмы снижения размерности, рассмотрим такие техники, как PCA и t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача снижения размерности — это задача преобразования данных с целью уменьшения количества признаков, которые описывают объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы можете догадаться, при уменьшении количества признаков мы теряем часть информации. Например, на изображении ниже Губка Боб после уменьшения размерности стал описываться меньшим количеством признаков и поэтому стал выглядеть по-другому.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод главных компонент, или PCA (Principal Components Analysis) — это один из базовых способов уменьшения размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный метод имеет широкое применение:\n",
    "\n",
    "Подавление шума на изображениях.\n",
    "\n",
    "Изображение состоит из пикселей, которые можно рассматривать как набор точек в многомерном пространстве. С помощью метода снижения размерности PCA можно преобразовать этот набор точек и оставить только первые компоненты, полученные после преобразования. В этих компонентах будет содержаться основная информация об изображении, но не будет шума. Таким образом мы улучшим качество изображения.\n",
    "\n",
    "Качество картинки с шумом и без него:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_27.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексация видео для быстрого поиска по базе.\n",
    "\n",
    "Каждый кадр видео можно преобразовать с помощью PCA и представить несколькими значениями. Далее эти значения легко хранить и искать в базе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Для уменьшения размерности метод главных компонент проводит линейное преобразование пространства, которое сохраняет длины векторов. Таким образом происходит отображение признаков в новое пространство с меньшей размерностью.\n",
    "\n",
    "В новом пространстве появляются новые оси. Они строятся таким образом, что для первой оси дисперсия данных должна быть максимальной, а вторая ось ортогональна первой и имеет максимально возможную дисперсию.\n",
    "\n",
    "Первой главной компонентой будет называться первая ось в новом пространстве. ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_29.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как запустить PCA с помощью библиотеки sklearn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# обучаем модель на данных X\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pca\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# применяем уменьшение размерности к матрице X\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pca\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# из модуля decomposition библиотеки sklearn импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# создаём объект класса PCA\n",
    "# n_components — задаём количество компонентов для проведения трансформации\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# обучаем модель на данных X\n",
    "pca.fit(X)\n",
    "# применяем уменьшение размерности к матрице X\n",
    "pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы можем сразу обучить модель и применить трансформацию уменьшения размерности:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим работу с PCA на практике. Из датасетов sklearn импортируем датасет MNIST — это данные, основанные на рукописном начертании цифр: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Скриншот](./img//Screenshot_30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для данного датасета обучим модель, которая по пикселям изображения предсказывает, что за цифра на нём изображена. Первой мы разработаем модель, которая будет учитывать все признаки, а затем уменьшим размерность данных с помощью PCA и ещё раз обучим модель. Далее мы сравним качество полученных моделей и время, которое было затрачено на обучение в каждом случае.\n",
    "\n",
    "В датасете MNIST представлено 70 000 изображений, каждое из которых описывается 784 признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим датасет MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_openml(\"mnist_784\")\n",
    "# загрузим признаки в переменную X  \n",
    "X = dataset['data']\n",
    "# загрузим «ответы» в переменную y\n",
    "y = dataset['target']\n",
    "# разделим данные с помощью sklearn на данные для обучения и теста\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "# импортируем StandardScaler для стандартизации данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# создадим объект класса StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# трансформируем датасеты train_x и test_x\n",
    "train_x = scaler.transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# создадим объект класса PCA\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(train_x)\n",
    "# уменьшим размерность данных\n",
    "train_x_pca = pca.transform(train_x)\n",
    "test_x_pca = pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько признаков описывало объект до и после уменьшения размерности. Как мы можем заметить, сначала было 787 признаков, а в конце объект описывают уже 300 главных компонент:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x[0]))\n",
    "print(len(train_x_pca[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии, которая на вход будет принимать пиксели изображения и предсказывать, что на нём нарисовано.\n",
    "\n",
    "Напишем функцию, которая будет принимать на вход данные для обучения (матрицу с признаками и правильные ответы) и данные для тестирования модели, а на выходе будет возвращать время, затраченное на обучение модели, и качество модели. В качестве метрики оценивания качества будем использовать метрику accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения 13.442823648452759, метрика модели 0.9251428571428572\n",
      "Модель, построенная на всех исходных признаках. Время обучения 17.64058780670166, метрика модели 0.9191428571428572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "# напишем функцию, которая на вход принимает X и y, а возвращает модель и время\n",
    "def get_time_and_accuracy(train_x, train_y, test_x, test_y):\n",
    "    # создадим объект класса LogisticRegression\n",
    "    log_reg_model = LogisticRegression(max_iter=1000)\n",
    "    # запишем время с начала эпохи в секундах до обучения модели\n",
    "    start_time = time()\n",
    "    # обучим модель\n",
    "    log_reg_model.fit(train_x, train_y)    \n",
    "    # запишем время с начала эпохи в секундах после обучения\n",
    "    end_time = time()\n",
    "    # подсчитаем время, потраченное на обучение модели\n",
    "    delta_time = end_time-start_time\n",
    "    # предскажем на тестовых данных\n",
    "    y_pred = log_reg_model.predict(test_x)\n",
    "    # посчитаем скор для тестового предсказания\n",
    "    score = accuracy_score(test_y, y_pred)\n",
    "    # вернём время, потраченное на обучение, и качество полученной модели\n",
    "    return delta_time, score\n",
    "\n",
    "model_pca_time, model_pca_acc = get_time_and_accuracy(train_x_pca, train_y, test_x_pca, test_y)\n",
    "print(f\"Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения {model_pca_time}, метрика модели {model_pca_acc}\")\n",
    "# Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения 54.12072825431824, метрика модели 0.9255714285714286\n",
    "\n",
    "model_time, model_acc = get_time_and_accuracy(train_x, train_y, test_x, test_y)\n",
    "print(f\"Модель, построенная на всех исходных признаках. Время обучения {model_time}, метрика модели {model_acc}\")\n",
    "# Модель, построенная на всех исходных признаках. Время обучения 108.04033303260803, метрика модели 0.9187142857142857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, в данном случае мы потратили на обучение модели в два раза меньше времени, а качество осталось практически таким же.\n",
    "\n",
    "В реальной работе бывает гораздо больше данных и на обучение модели уходит отнюдь не две минуты. Таким образом, применив технику уменьшения размерности, можно существенно сэкономить время.\n",
    "\n",
    "→ Мы научились ускорять обучение моделей с помощью понижения размерности данных, а теперь давайте научимся визуализировать многомерное пространство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы визуализировать многомерное пространство признаков, необходимо уменьшить его размерность до двухмерного или трёхмерного. В этом поможет t-SNE (t-distributed Stochastic Neighbor Embedding), что переводится с английского как «стохастическое вложение соседей с t-распределением». "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE, в отличие от PCA, использует технику нелинейного снижения размерности данных. Обычно используется снижение размерности многомерного пространства до двух- или трёхмерного с целью дальнейшей визуализации. При преобразовании похожие объекты оказываются рядом, а непохожие — далеко друг от друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим t-SNE на примере понижения размерности двухмерного пространства до одномерного.\n",
    "\n",
    "Если у нас есть такое распределение точек, как на графике ниже,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "то, казалось бы, чтобы уменьшить размерность данных, нужно просто спроецировать эти точки на ось x:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но, как мы видим, розовый и синий кластеры перестают быть различными кластерами, когда мы проецируем данные на ось X — данные перемешались. t-SNE позволяет не допускать такого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Алгоритм состоит из следующих шагов: ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке выше показано, как для одной точки рассчитали расстояние до всех других точек датасета, далее получили схожесть объектов с помощью нормального распределения и построили матрицу. На данной матрице по строкам и столбцам находятся объекты датасета, а в ячейках — значения схожести двух объектов. Красным обозначена похожесть объекта на самого себя (в таком случае это значение максимально), а розовым — объекты, которые имеют большую схожесть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С каждой итерацией точки перемещаются к своим ближайшим соседям из исходного многомерного пространства и удаляются от отдалённых:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы итеративно приходим к разделению объектов в новом пространстве.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа алгоритма выглядит так:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_36.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_38.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первой итерации данные расположены хаотично, но с каждой итерацией похожие объекты подходят ближе друг к другу, а непохожие отдаляются друг от друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Как запустить t-SNE? ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс TSNE из модуля manifold библиотеки sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# создаём объект класса TSNE\n",
    "# n_components — размерность нового пространства\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "# обучаем модель на данных X и производим трансформацию\n",
    "tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем уменьшить размерность и визуализировать пространство пикселей, которые описывают данные рукописного начертания цифр. Сравним, какая визуализация получается при использовании PCA и tSNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_40.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, алгоритм t-SNE отлично справляется с уменьшением размерности для визуализации, а вот при использовании PCA данные не разделились на кластеры и пересекаются друг с другом — такую визуализацию будет неудобно анализировать.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном юните мы изучили несколько техник уменьшения размерности данных. Эти методы используются для разных целей. Давайте кратко подведём итоги и составим сводную таблицу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Скриншот](./img//Screenshot_41.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
