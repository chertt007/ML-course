{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Для начала давайте вспомним, что такое задача регрессии в машинном обучении.\n",
    "\n",
    "Регрессия — это класс задач обучения с учителем, когда по определённому набору признаков объекта необходимо предсказать числовую целевую переменную.\n",
    "\n",
    "Цель обучения — построить модель, которая бы отражала зависимость между признаками и целевой числовой переменной.\n",
    "\n",
    "Когда зависимость принимается линейной, такая модель называется линейной регрессией.\n",
    "\n",
    "img\n",
    "\n",
    "ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛИНЕЙНОЙ РЕГРЕССИИ\n",
    "\n",
    "Линейная регрессия (Linear Regression) — одна из простейших моделей для решения задачи регрессии. Главная гипотеза состоит в том, что рассматриваемая зависимость является линейной.\n",
    "\n",
    "Общий вид модели в случае, когда целевая переменная зависит от  факторов, будет иметь следующий вид:\n",
    "\n",
    "Давайте разбираться, что в этом выражении значит каждая из переменных. Начнём с простого — с двумерного случая.\n",
    "\n",
    "2D-случай\n",
    "\n",
    "Для начала поговорим о самом простом случае, когда у нас есть один фактор и зависящий от него целевой признак. Геометрически такая зависимость представляет собой координатную плоскость, где мы отмечаем точки по оси x и соответствующие им точки на оси y.\n",
    "\n",
    "Рассмотрим задачу из нефтяной отрасли. Есть набор данных, где представлены данные о средней пористости скважин (в процентах) и добыче газа на этих скважинах в сутки (в миллионах кубических футов). \n",
    "\n",
    "Нам бы хотелось построить модель, которая опишет зависимость и позволит по известной пористости скважин предсказывать неизвестную выработку газа.\n",
    "\n",
    "Зависимость целевого признака от фактора представлена на диаграмме рассеяния (см. ниже). Пористость скважины отложена по оси абсцисс — Porosity (%), а добыча газа — по оси ординат, Gas production (Mcf/day).\n",
    "\n",
    "img\n",
    "\n",
    "Из диаграммы отчётливо видно, что с ростом пористости скважины растёт добыча газа. Причём растёт она преимущественно линейно: основная масса точек находится на одной прямой.\n",
    "\n",
    "Идея! Давайте проведём через точки прямую линию так, чтобы она максимально хорошо описывала зависимость.\n",
    "\n",
    "Для этого сначала вспомним уравнение прямой из школьного курса математики:\n",
    "\n",
    "где:\n",
    "\n",
    " — это некоторый фактор, от которого зависит целевая переменная . В нашем случае,  — это пористость скважины, а  — добыча газа.\n",
    " — коэффициент наклона прямой (тангенс угла наклона). Если , это означает, что угол наклона прямой острый и прямая возрастает. Если , угол наклона тупой и прямая убывает.\n",
    " — коэффициент смещения прямой по оси . Он будет соответствовать значению  при . То есть это точка пересечения прямой и оси Y.\n",
    "img\n",
    "\n",
    "На данном графике изображены две прямые с разными коэффициентами наклона. Зелёная прямая соответствует положительному значению , и геометрически  равен тангенсу острого угла  наклона прямой по отношению к оси x: . Синяя прямая соответствует отрицательному значению , и геометрически  равен тангенсу тупого угла   наклона прямой по отношению к оси x: . Каждая из прямых пересекается с осью y в точках  и  — это и есть коэффициент смещения прямых.\n",
    "\n",
    "Это уравнение и есть двумерная модель линейной регрессии. Зная коэффициенты  и , мы можем подставить в него любую пористость скважины x и получить предсказание добычи газа y.\n",
    "\n",
    "Однако в машинном обучении приняты немного другие обозначения. Фактическое значение целевой переменной обозначается как , а вот предсказанное моделью — . Также для удобства коэффициенты  и  приведём к единому обозначению:  и . Тогда уравнение модели линейной регрессии запишется в виде:\n",
    "\n",
    "Примечание. Коэффициенты  и  называются параметрами линейной регрессии.\n",
    "\n",
    "Остаётся только один вопрос: откуда, собственно, взять параметры  и ? Обсудим этот вопрос чуть позже.\n",
    "\n",
    "А пока представим, что параметры мы нашли. В таком случае можно построить прямую, которая опишет нашу зависимость. Пусть коэффициенты составляют (мы их нашли сами по методу наименьших квадратов, о котором поговорим ниже):\n",
    "\n",
    "Тогда модель будет иметь следующий вид:\n",
    "\n",
    "Если подставлять значения конкретные значения пористости  в модель, можно построить прямую, которая описывает исходную зависимость. Это и будет графическая интерпретация нашей модели:\n",
    "\n",
    "img\n",
    "\n",
    "3D-случай\n",
    "\n",
    "Теперь представим, что у нас не один фактор, а два. Например, помимо пористости скважины, мы дополнительно знаем ещё и о её хрупкости в процентах. То есть у нас теперь есть два фактора:  — пористость и  — хрупкость.\n",
    "\n",
    "Можно отобразить такую зависимость добычи газа от этих факторов в трёхмерном пространстве в виде диаграммы рассеяния:\n",
    "\n",
    "img\n",
    "\n",
    "В таком случае в выражение для модели добавится ещё одна переменная  и соответствующий ей коэффициент :\n",
    "\n",
    "Опять же, представим, что параметры модели мы нашли и они равны:\n",
    "\n",
    "Тогда модель будет иметь следующий вид:\n",
    "\n",
    "Это была алгебра — теперь перейдём к геометрии. Геометрически данное уравнение описывает плоскость в трёхмерном пространстве с осями  и ,  — смещение плоскости по вертикальной оси, а коэффициенты  и  — коэффициенты наклона этой плоскости к осям  и . \n",
    "\n",
    "То есть это будет плоскость, которая подстроена под точки в трёхмерном пространстве:\n",
    "\n",
    "img\n",
    "\n",
    "Общий случай\n",
    "\n",
    "А что если факторов не два, а больше: 3, 15, 100? Тут-то мы и приходим к общему виду модели линейной регрессии, который вводили в самом начале. Пусть у нас есть  факторов , от которых зависит целевая переменная .\n",
    "\n",
    "В геометрическом смысле данное уравнение описывает плоскость в -мерном пространстве ( факторов +  целевой признак отложены по осям координат). Такую плоскость называют гиперплоскостью.\n",
    "\n",
    "Абстрактное -мерное пространство, конечно же, невозможно отобразить графически и сложно даже представить, как оно выглядит. Но нам это и не нужно. Все операции в таком пространстве аналогичны операциям в двумерном или трёхмерном пространстве.\n",
    "\n",
    "→ Для понимания принципа работы мы будем рассматривать только прямую в двумерном пространстве, а результат уже обобщать на случай с большей размерностью.\n",
    "\n",
    "Стоит отметить, что в DS мы, как правило, работаем с большим количеством факторов (больше двух), которые описывают данные, поэтому отобразить модель в геометрическом пространстве не получится, но важно понимать, что представляет собой сама модель.\n",
    "\n",
    "ПОИСК ПАРАМЕТРОВ ЛИНЕЙНОЙ РЕГРЕССИИ: МЕТОД НАИМЕНЬШИХ КВАДРАТОВ\n",
    "\n",
    "Теперь мы знаем, как выглядит модель линейной регрессии в общем случае: это простое линейное выражение, подставляя в которое значения факторов, можно найти целевую переменную. Это линейное выражение соответствует прямой, плоскости или гиперплоскости в зависимости от количества признаков.\n",
    "\n",
    "Остаётся вопрос: откуда взять коэффициенты, которые стоят при ?\n",
    "\n",
    "Прямую же можно провести как угодно. Вот например несколько прямых, построенных с различными случайными коэффициентами:\n",
    "\n",
    "img\n",
    "\n",
    "Какие параметры будут наилучшими?\n",
    "\n",
    "Для ответа на этот вопрос давайте вспомним схему обучения моделей машинного обучения по принципу минимизации эмпирического риска, которую мы рассматривали в предыдущем модуле:\n",
    "\n",
    "img\n",
    "\n",
    "Согласно данной схеме обучения, поиск параметров производится путём минимизации некоторой функции ошибки. Математически мы пытаемся с помощью методов оптимизации найти такие параметры, чтобы ошибка была наименьшей из возможных.\n",
    "\n",
    "Осталось только понять: где взять эту функцию ошибки? Ответ кроется в картинке ниже. Давайте представим, как могла бы выглядеть прямая в двумерном пространстве, проведённая, например, через пять точек:\n",
    "\n",
    "img\n",
    "\n",
    "Что вообще есть ошибка? В самом простом понимании это расхождение между истиной и предсказанием.\n",
    "\n",
    "Чтобы не учитывать знак расхождения, можно взять модуль разницы между истинным значением и предсказанным (тем, что лежит на прямой). Рассчитать ошибки   (на рисунке они отмечены красными отрезками) для всех пяти точек можно следующим образом:\n",
    "\n",
    "где  — это результат подстановки -ого значения  в модель линейной регрессии.\n",
    "\n",
    "Вычислим среднее по всем ошибкам. Такая ошибка называется средняя абсолютная ошибка (Mean Absolute Error, MAE) и записывается следующим образом (в двумерном случае):\n",
    "\n",
    "Осталось только найти такие  и , при которых MAE была бы минимальной. В математике это записывается следующим образом:\n",
    "\n",
    "→ Тут-то математики и столкнулись с проблемой. Оказывается, если пытаться решить эту оптимизационную задачу классическими способами (через условия экстремума функции), то поиск решения будет противоречить основным законам математического анализа. Почему? Обсудим это в модулях по математике, когда будем решать оптимизационные задачи.\n",
    "\n",
    "ОТВЕТ ДЛЯ ЛЮБОЗНАТЕЛЬНЫХ\n",
    "Но математикам, конечно, удалось найти выход. Вместо модуля можно использовать квадрат — он тоже убирает знак ошибки и по сути аналогичен модулю. Получим среднеквадратичную ошибку (Mean Square Error, MSE):\n",
    "\n",
    "Это и будет наша функция ошибки, которую мы будем минимизировать, управляя параметрами  и :\n",
    "\n",
    "Примечание. В общем случае, когда X — это таблица из  наблюдений и  признаков, постановка задачи оптимизации MSE выглядит следующим образом:\n",
    "\n",
    "где  — это значение, которое находится в -ой строке и -ом столбце таблицы наблюдений.\n",
    "\n",
    "Математике известно решение данной задачи оптимизации. Метод поиска параметров линейной регрессии называется методом наименьших квадратов (сокращённо — МНК) и был изобретён Гауссом ещё в 1795 году. В английской литературе часто можно встретить аббревиатуру OLS (Ordinary Least Squares).\n",
    "\n",
    "→ Решать саму задачу поиска минимума функции мы сейчас не будем, так как пока что не владеем достаточными для её решения знаниями о частной производной и условиях экстремума функции многих переменных, но приведём финальный ответ, полученный для общего случая.\n",
    "\n",
    "Итак, пусть у нас есть матрица X, в которой по строкам собрано  наблюдений, а по столбцам отложено  факторов — по сути, это обычный, привычный нам DataFrame. К каждому примеру из таблицы X есть ответ y.\n",
    "\n",
    "Зависимость между факторами и целевым признаком принята линейной, то есть рассматривается обучение модели линейной регрессии:\n",
    "\n",
    " — вектор параметров\n",
    "\n",
    " — вектор признаков\n",
    "\n",
    "Мы хотим найти наилучшую оценку для , , , ..., .\n",
    "\n",
    "Примечание. Для того чтобы конечная запись формулы была короче и можно было включить в вектор  коэффициент смещения прямой , в матрицу X первым добавляют столбец, полностью состоящий из единиц. Это связано со спецификой матричного умножения, о котором мы поговорим далее в курсе.\n",
    "\n",
    "Согласно методу наименьших квадратов, аналитическое выражение для поиска вектора коэффициентов уравнения линейной регрессии имеет вид:\n",
    "\n",
    "Данная матричная формула позволяет найти неизвестные параметры линейной регрессии в виде вектора . Найденные коэффициенты называют решением задачи линейной регрессии.\n",
    "\n",
    "Примечание. Верхний индекс T у матрицы X означает транспонирование матриц — смену строк и столбцов местами (поворот таблицы). Пример:\n",
    "\n",
    "Операция возведения матриц в степень -1 называется обращением матриц. Полученная в результате матрица называется обратной к исходной. Так, матрица  является обратной к матрице .\n",
    "\n",
    "Саму процедуру обращения матриц мы будем рассматривать в модуле по линейной алгебре. Сейчас же для проведения этой операции мы будем использовать библиотеку numpy, которая позволяет очень быстро и просто обращать матрицы."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
